---
title: "DSO 530 Homework3 - Yurong Jiang"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#5.4 Exercises 1 
answer:

$$Var(\alpha X + (1-\alpha )Y) =\alpha ^2{\sigma_X^2}+ (1-\alpha)^2{\sigma_Y^2}+2\alpha(1-\alpha){\sigma_{XY}}$$
$$=\alpha ^2{\sigma_X^2}+{\sigma_Y^2}-2\alpha{\sigma_Y^2}+\alpha ^2{\sigma_Y^2}+2\alpha{\sigma_{XY}}-2\alpha^2{\sigma_{XY}}$$
$$=({\sigma_X^2}+{\sigma_Y^2}-2{\sigma_{XY}})\alpha ^2+(2{\sigma_{XY}}-2{\sigma_Y^2})\alpha+{\sigma_Y^2}$$

Then, taking the derivative with respect to $\alpha$ and set it eaqual to 0

$$2({\sigma_X^2}+{\sigma_Y^2}-2{\sigma_{XY}})\alpha +(2{\sigma_{XY}}-2{\sigma_Y^2})=0$$
Since assuming X and Y are not equal, and $${\sigma_X^2}+{\sigma_Y^2}-2{\sigma_{XY}}=Var(X - Y)>0$$


So, can get the global minimum of $Var(\alpha X + (1-\alpha )Y)$ at
$$\alpha = \frac{\sigma_Y^2-\sigma_{XY}}{\sigma_X^2+\sigma_Y^2-2\sigma_{XY}}$$


#5.4 Exercises 3
### (a) Explain how k-fold cross-validation is implemented.
With k-fold cross-validation, the dataset was equally and randomly divided into K groups. Every time, we use the i (i from 1 to k) group as the validation set and all other groups are training set and get a mean squared test error. k-fold cross-validation totally repeated k times, so the performance of the model is measured by the average of all the test errors.

###(b) What are the advantages and disadvantages of k-fold cross-validation relative to:
###i.The validation set approach?
Advantages: k-fold cross-validation is more reliable than validation set approach because  k-fold cross-validation will use all data but validation set approach will only use part of data as training data. Also because of this, K-fold cross-validation eliminates overestimate of  validation set approach.
Disadvantages: k-fold cross-validation needs more computational and costs more time.

###ii.LOOCV?
Advantages:k-fold cross-validation needs less computational and costs less time. Besides, K-fold cross-validation have higher bias and lower variance.

#5.4 Exercises 5
###In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.

###(b)Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:
```{r}
library(ISLR)
attach(Default)
totalrow=dim(Default)[1]
set.seed(1)
train=sample(totalrow, totalrow/2)
mlr.fit = glm(default ~ income + balance, data = Default, subset = train, family = 'binomial')
mlr.pred = predict(mlr.fit, Default[-train,], type = 'response')
mlr.validation = ifelse(mlr.pred > 0.5, 'Yes', 'No')
validation_error = mean(mlr.validation != Default[-train,]$default)
print(validation_error)
```

### (c)Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.
```{r}
for(i in 1:3){
train=sample(totalrow, totalrow/2)
mlr.fit = glm(default ~ income + balance, data = Default, subset = train, family = 'binomial')
mlr.pred = predict(mlr.fit, Default[-train,], type = 'response')
mlr.validation = ifelse(mlr.pred > 0.5, 'Yes', 'No')
validation_error = mean(mlr.validation != Default[-train,]$default)
print(validation_error)
}
```

When we using three different splits of the observation, we used different datasets to train the models and get different models, so get different test errors.

#8.4 Exercises 5
### Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of P(Class is Red|X):
###                           0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75.
### There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches?

Majority vote approach: choose Red for this class.
```{r}
prob = c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
ifelse((sum(prob>=0.5)/length(prob))>0.5, "Red",
ifelse((sum(prob>=0.5)/length(prob))==0.5, "Draw", "Green"))
```
Average Probility approach: choose Green for this class.
```{r}
prob = c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
ifelse(mean(prob)>0.5, "Red",
ifelse(mean(prob)==0.5, "Draw", "Green"))
```
#8.4 Exercises 8
### (a) Split the data set into a training set and a test set.
```{r}
library(tree)
attach(Carseats)
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
```

### (b) Fit a regression tree to the training set. Plot the tree, and inter- pret the results. What test MSE do you obtain?
```{r}
tree.carseats=tree(Sales~., Carseats, subset=train)
summary(tree.carseats)
tree.pred=predict(tree.carseats, Carseats[-train,])
plot(tree.carseats)
text(tree.carseats, pretty=0)
MSE_test=mean((tree.pred - Carseats.test$Sales)^2)
MSE_test
```
Test MSE is 4.844991

### (c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
```{r}
set.seed(1)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(cv.carseats$size[tree.min], cv.carseats$dev[tree.min], col = "red")
prune.carseats=prune.tree(tree.carseats,best=12)
prune.carseats
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats[-train,])
MSE_test=mean((tree.pred-Carseats.test$Sales)^2)
MSE_test 
```
Test MSE is 4.67. Yes, pruning the tree improves the test MSE.

###(d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.

```{r}
library(randomForest)
set.seed(1)
bag.randomForest=randomForest(Sales~.,data=Carseats,subset=train,mtry=10,importance =TRUE)
bag.randomForest
yhat.bag= predict(bag.randomForest, newdata=Carseats[-train ,])
MSE_test_bag= mean((yhat.bag - Carseats.test$Sales)^2)
MSE_test_bag
importance(bag.randomForest)
```

ShelveLoc and Price are two most important variables. 

### (e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which vari- ables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.
```{r}
set.seed(1)
rf.randomForest=randomForest(Sales~.,data=Carseats,subset=train,mtry=3,importance =TRUE)
rf.randomForest
yhat.rf= predict(rf.randomForest, newdata=Carseats[-train ,])
MSE_test_rf= mean((yhat.rf-Carseats.test$Sales)^2)
MSE_test_rf
importance(rf.randomForest)
```
ShelveLoc and Price are two most important variables. ShelveLoc and Price are the two most important variables. When we increase m, random forest will improve bias and increase variance.
